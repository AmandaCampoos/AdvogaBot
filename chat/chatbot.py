import os
import streamlit as st
from langchain_aws.embeddings import BedrockEmbeddings
from langchain_aws.llms import BedrockLLM
from langchain_chroma import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import boto3

# Configura√ß√£o da p√°gina
st.set_page_config(page_title="Assistente Jur√≠dico", page_icon="‚öñÔ∏è")

# T√≠tulo da aplica√ß√£o
st.title("‚öñÔ∏è Assistente Jur√≠dico")

# Inicializa√ß√£o dos componentes RAG
def initialize_system():
    try:
        # Configura√ß√£o do cliente Bedrock
        bedrock_client = boto3.client(
            service_name="bedrock-runtime",
            region_name="us-east-1",
            aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
            aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
            aws_session_token=os.getenv("AWS_SESSION_TOKEN")
        )

        # Configura√ß√£o do ChromaDB para recupera√ß√£o
        embeddings = BedrockEmbeddings(
            client=bedrock_client,
            model_id="amazon.titan-embed-text-v2:0"
        )
        vectorstore = Chroma(
            persist_directory="/rag_juridico/chroma_db/chroma.sqlite3",
            embedding_function=embeddings
        )

        return vectorstore, bedrock_client
    except Exception as e:
        raise RuntimeError(f"Erro ao inicializar o sistema: {str(e)}")

# Inicializar os componentes no in√≠cio
vectorstore, bedrock_client = initialize_system()

# Fun√ß√£o para processar a consulta
def process_query(user_query):
    try:
        # Recupera documentos relevantes do ChromaDB
        retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3})
        docs = retriever.invoke(user_query)

        context = "\n\n".join([doc.page_content for doc in docs])

        # Cria√ß√£o da mensagem no formato esperado
        input_text = f"""
        Voc√™ √© um assistente jur√≠dico especializado em consulta de documentos legais. 
        Baseie suas respostas estritamente nos documentos fornecidos.
        Se n√£o encontrar documentos relevantes, responda somente com "Desculpe, n√£o consegui encontrar informa√ß√µes relevantes nos documentos fornecidos." e n√£o mostre mais nada.
        Pergunta: {user_query}
        Contexto: {context}
        """

        body = {
            "inputText": input_text,
            "textGenerationConfig": {
                "maxTokenCount": 8192,
                "stopSequences": [],
                "temperature": 0,
                "topP": 1
            }
        }

        # Envia a mensagem para o Bedrock usando o cliente boto3
        response = bedrock_client.invoke_model( 
            modelId="amazon.titan-text-express-v1",  
            body=json.dumps(body),  
            contentType="application/json", 
            accept="application/json" 
        )

        # A resposta vir√° em formato JSON
        response_content = json.loads(response['body'].read().decode('utf-8'))

        # Retorna a resposta e os documentos de origem
        generated_text = response_content.get("results", [{}])[0].get("outputText", "Sem resposta.")

        # Retorna a resposta e os documentos de origem
        return generated_text, docs
    except Exception as e:
        raise ValueError(f"Erro ao processar a consulta: {str(e)}")


# Interface do chat simplificada e robusta
def main():
    st.markdown("""
    Sistema de consulta jur√≠dica com RAG usando:
    - **Amazon Bedrock** (Claude v2)
    - **ChromaDB** para busca vetorial
    """)
    
    qa_chain = init_rag_system()
    
    if qa_chain is None:
        st.error("‚ö†Ô∏è O sistema n√£o p√¥de ser inicializado. Verifique os logs para detalhes.")
        return
    
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    if prompt := st.chat_input("Fa√ßa sua pergunta jur√≠dica"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        with st.chat_message("user"):
            st.markdown(prompt)
        
        with st.chat_message("assistant"):
            with st.spinner("Processando consulta..."):
                try:
                    result = qa_chain.invoke({"query": prompt})
                    response = result["result"]
                    
                    st.markdown(response)
                    st.session_state.messages.append({"role": "assistant", "content": response})
                    
                    with st.expander("üìÑ Ver documentos de refer√™ncia"):
                        for doc in result["source_documents"]:
                            st.markdown(f"**Fonte:** {doc.metadata.get('source', 'Desconhecida')}")
                            st.markdown(f"**Trecho relevante:**\n{doc.page_content[:300]}...")
                
                except Exception as e:
                    st.error(f"Erro ao processar sua pergunta: {str(e)}")

if __name__ == "__main__":
    main()